{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc-autonumbering":true,"toc-showcode":false,"toc-showmarkdowntxt":false,"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7375495,"sourceType":"datasetVersion","datasetId":4285675}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/diaconumadalina/summarization-reviews?scriptVersionId=158488335\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## 2 Setup","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Import and configure libraries ","metadata":{}},{"cell_type":"code","source":"# Data manipulation libraries\nimport pandas as pd\nimport spacy\nfrom spacy import displacy # is used for visualizing the dependency parse tree and named entity recognition (NER) annotations.\n\n# General Imports\nimport time\n\n# Data modeling libraries\nfrom sklearn.model_selection import train_test_split\n\n# text processing and cleaning\nimport re # This line imports the regular expression (regex) module, which provides functions for working with regular expressions. \nimport nltk\nfrom nltk.stem import WordNetLemmatizer #  is used for lemmatization, which is the process of reducing words to their base or root form: from running to run\nfrom nltk.corpus import stopwords #The stopwords corpus from NLTK contains common words that are often removed from text during text preprocessing/ These words (like 'and', 'the', 'is', etc.) are considered as noise in many natural language processing tasks \n\n# Display in jupyter\n# from IPython.core.display import display, HTML\n# # Set the width of the output cell\n# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:25.323843Z","iopub.execute_input":"2024-01-10T11:11:25.324336Z","iopub.status.idle":"2024-01-10T11:11:25.331914Z","shell.execute_reply.started":"2024-01-10T11:11:25.324302Z","shell.execute_reply":"2024-01-10T11:11:25.330678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Constants and helper functions ","metadata":{}},{"cell_type":"markdown","source":"#### Constants","metadata":{}},{"cell_type":"markdown","source":"#### Helper functions","metadata":{}},{"cell_type":"code","source":"def load_dataset_from_json(json_file_path):\n    \"\"\"\n    :param json_file_path (str) :Path to the JSON file.\n    :return: pd.DataFrame: DataFrame containing the loaded data.\n    \"\"\"\n    df = pd.read_json(json_file_path)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:25.334182Z","iopub.execute_input":"2024-01-10T11:11:25.33475Z","iopub.status.idle":"2024-01-10T11:11:25.353928Z","shell.execute_reply.started":"2024-01-10T11:11:25.334692Z","shell.execute_reply":"2024-01-10T11:11:25.352718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Set dataframe ","metadata":{}},{"cell_type":"code","source":"df = load_dataset_from_json(\"/kaggle/input/amazon-one-plus-reviews/amazon_one_plus_reviews.json\")\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:25.356126Z","iopub.execute_input":"2024-01-10T11:11:25.358121Z","iopub.status.idle":"2024-01-10T11:11:26.261191Z","shell.execute_reply.started":"2024-01-10T11:11:25.35807Z","shell.execute_reply":"2024-01-10T11:11:26.259835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# conda update --all","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:26.26284Z","iopub.execute_input":"2024-01-10T11:11:26.263223Z","iopub.status.idle":"2024-01-10T11:11:26.268875Z","shell.execute_reply.started":"2024-01-10T11:11:26.26319Z","shell.execute_reply":"2024-01-10T11:11:26.26768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:26.271664Z","iopub.execute_input":"2024-01-10T11:11:26.27217Z","iopub.status.idle":"2024-01-10T11:11:26.370752Z","shell.execute_reply.started":"2024-01-10T11:11:26.272136Z","shell.execute_reply":"2024-01-10T11:11:26.369488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:26.372008Z","iopub.execute_input":"2024-01-10T11:11:26.372358Z","iopub.status.idle":"2024-01-10T11:11:28.354361Z","shell.execute_reply.started":"2024-01-10T11:11:26.372328Z","shell.execute_reply":"2024-01-10T11:11:28.353082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The dataset from worlbank contains information about 3 types of produc in our application we need for startjust one product\ndf['product'].value_counts()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-01-10T11:11:28.356443Z","iopub.execute_input":"2024-01-10T11:11:28.357172Z","iopub.status.idle":"2024-01-10T11:11:28.371735Z","shell.execute_reply.started":"2024-01-10T11:11:28.357138Z","shell.execute_reply":"2024-01-10T11:11:28.370777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = df['product'] == 'Redmi Note 8 (Neptune Blue, 4GB RAM, 64GB Storage)'\ndf = df[mask].reset_index(drop=True)\ndf = df[['reviewed_at', 'review_text', 'review_title']]\ndf = df.rename(columns = {'review_title' : 'Summary', 'review_text' : 'Review', 'reviewed_at' : 'Date'})","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.373055Z","iopub.execute_input":"2024-01-10T11:11:28.373418Z","iopub.status.idle":"2024-01-10T11:11:28.410985Z","shell.execute_reply.started":"2024-01-10T11:11:28.373387Z","shell.execute_reply":"2024-01-10T11:11:28.409755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Format the date as \"30 August 2021\"\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%d %B %Y') if not pd.isnull(x) else '')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.412291Z","iopub.execute_input":"2024-01-10T11:11:28.412681Z","iopub.status.idle":"2024-01-10T11:11:28.552316Z","shell.execute_reply.started":"2024-01-10T11:11:28.412649Z","shell.execute_reply":"2024-01-10T11:11:28.55105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3 EDA","metadata":{}},{"cell_type":"markdown","source":"##### For this analyse I will use the product named Redmi Note 8 (Neptune Blue, 4GB RAM, 64GB Storage) 13934\n","metadata":{}},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.558047Z","iopub.execute_input":"2024-01-10T11:11:28.558492Z","iopub.status.idle":"2024-01-10T11:11:28.570089Z","shell.execute_reply.started":"2024-01-10T11:11:28.558457Z","shell.execute_reply":"2024-01-10T11:11:28.569042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.571813Z","iopub.execute_input":"2024-01-10T11:11:28.572489Z","iopub.status.idle":"2024-01-10T11:11:28.598059Z","shell.execute_reply.started":"2024-01-10T11:11:28.572453Z","shell.execute_reply":"2024-01-10T11:11:28.596859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4 Training Setup","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Train, Test split","metadata":{}},{"cell_type":"markdown","source":"##### Train, Test split","metadata":{}},{"cell_type":"code","source":"X = df.drop([\"Summary\"], axis =1 )\nX_train, X_test = train_test_split(X , test_size = 0.2, random_state = 0)\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.600152Z","iopub.execute_input":"2024-01-10T11:11:28.601266Z","iopub.status.idle":"2024-01-10T11:11:28.623204Z","shell.execute_reply.started":"2024-01-10T11:11:28.601226Z","shell.execute_reply":"2024-01-10T11:11:28.622007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Sentence Tokenization\n","metadata":{}},{"cell_type":"code","source":"def split_review_custom_delimiters(text):\n    \"\"\"\n    This function splits the review into multiple sentences based on custom delimiters.\n    \n    Args:\n        text (str): The input text to be split.\n    Returns:\n        list: A list of sentences after splitting based on the specified custom delimiters.\n    \"\"\"\n    delimiters = \".\", \"but\", \"and\", \"also\"\n    escaped_delimiters = map(re.escape, delimiters) # Result: ['\\\\.', 'but', 'and', 'also']\n    regex_pattern = '|'.join(escaped_delimiters) # Applying the custom delimiters # Result: '\\\\.|but|and|also'\n    splitted = re.split(regex_pattern, text) # Splitting the review function from the re module to split the input text into a list of substrings based on the specified regular expression pattern.\n    return[sentence.strip() for sentence in splitted if sentence.strip()] #this line ensures that only non-empty sentences (after stripping whitespaces) are included in the final result.  sentence.strip(): Strips any leading or trailing whitespaces from the sentence.","metadata":{"execution":{"iopub.status.busy":"2024-01-10T17:10:27.078719Z","iopub.execute_input":"2024-01-10T17:10:27.079175Z","iopub.status.idle":"2024-01-10T17:10:27.086704Z","shell.execute_reply.started":"2024-01-10T17:10:27.079138Z","shell.execute_reply":"2024-01-10T17:10:27.085539Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"##### Data cleaning","metadata":{}},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.639077Z","iopub.execute_input":"2024-01-10T11:11:28.639636Z","iopub.status.idle":"2024-01-10T11:11:28.658087Z","shell.execute_reply.started":"2024-01-10T11:11:28.639587Z","shell.execute_reply":"2024-01-10T11:11:28.657069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemma = WordNetLemmatizer()\nall_stopwords = set(stopwords.words('english'))\n\ncustom_stopwords = ['not', 'but', 'because', 'against', 'between', 'up', 'down', 'in', 'out', 'once', 'before','after', 'few', 'more', 'most', 'no', 'nor', 'same', 'some']\n\nfor stopword in custom_stopwords:\n    all_stopwords.remove(stopword)\n\ndef clean_aspect_spacy(reviews):\n    \"\"\"\n    this function prepares text for analysis by cleaning it, making it more consistent, \n    and removing elements that may not carry substantial meaning for downstream tasks in natural language processing as punctuations, stopwords, and other non-alphanumeric characters.\n    It expands contractions and replaces some words with an empty string.\n    \n    Args:\n        reviews (str): The text to be cleaned.\n        lemma (WordNetLemmatizer): An instance of WordNetLemmatizer for lemmatization.\n        all_stopwords (set): A set of stopwords to be removed from the text.\n\n    Returns:\n        str: The cleaned and preprocessed text.\n        \n    \"\"\"\n    text = reviews.lower()\n    \n    contractions = {\n        \"won't\": \"will not\",\n        \"cannot\": \"can not\",\n        \"can't\": \"can not\",\n        \"n't\": \" not\",\n        \"what's\": \"what is\",\n        \"it's\": \"it is\",\n        \"'ve\": \" have\",\n        \"i'm\": \"i am\",\n        \"'re\": \" are\",\n        \"he's\": \"he is\",\n        \"she's\": \"she is\",\n        \"*****\": \" \",        \n    }\n    \n    for contraction, expansion in contractions.items():\n        text = text.replace(contraction, expansion)\n        \n    # Remove special characters, numbers, and extra spaces.\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n    text = re.sub(' +', ' ', text)\n    \n    # Lemmatization and removing stopwords\n    words = text.split()\n    cleaned_words = [lemma.lemmatize(word) for word in words if word not in set(all_stopwords)]\n    \n    # Join the cleaned words back into a sentence\n    cleaned_text = ' '.join(cleaned_words) \n    \n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.659468Z","iopub.execute_input":"2024-01-10T11:11:28.660654Z","iopub.status.idle":"2024-01-10T11:11:28.675378Z","shell.execute_reply.started":"2024-01-10T11:11:28.660616Z","shell.execute_reply":"2024-01-10T11:11:28.673798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Reconstruct the DataFrame","metadata":{}},{"cell_type":"code","source":"nltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.677378Z","iopub.execute_input":"2024-01-10T11:11:28.678618Z","iopub.status.idle":"2024-01-10T11:11:28.69536Z","shell.execute_reply.started":"2024-01-10T11:11:28.678565Z","shell.execute_reply":"2024-01-10T11:11:28.693686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install --upgrade nltk","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:46.386582Z","iopub.execute_input":"2024-01-10T11:11:46.386986Z","iopub.status.idle":"2024-01-10T11:11:46.392356Z","shell.execute_reply.started":"2024-01-10T11:11:46.386956Z","shell.execute_reply":"2024-01-10T11:11:46.391063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_and_clean_reviews(df):\n    \"\"\"\n    This function takes a DataFrame 'df' with 'Review' and 'Date' columns, splits each review into smaller components,\n    filters out components with fewer than three words, and applies a text cleaning function to each split and cleaned review.\n    \n    Parameters:\n    - df (DataFrame): Input DataFrame with 'Review' and 'Date' columns.\n\n    Returns:\n    DataFrame: A new DataFrame with 'Date' and 'Review' columns, where each review has been split, filtered, and cleaned.\n    \"\"\"\n    reviews = []\n    dates = []\n    \n    for i, review_text in enumerate(df[\"Review\"].values):\n        review_split = split_review_custom_delimiters(review_text)\n        \n        # Filter out components with fewer than three words\n        review_split_filtered = [split for split in review_split if len(split.split()) >= 3]\n    \n        # Duplicate dates as string for the corresponding split reviews\n        duplicate_dates = [str(df[\"Date\"].values[i]) for _ in range(len(review_split_filtered))]\n\n        # Extend the lists with split and duplicated reviews and dates\n        reviews.extend(review_split_filtered)\n        dates.extend(duplicate_dates)\n\n    # Apply the text cleaning function to each split and cleaned review    \n    cleaned_reviews = [clean_aspect_spacy(text) for text in reviews]\n    \n    # Create a new DataFrame with 'Date' and 'Review' columns\n    data = pd.DataFrame({\"Date\": dates, \"Review\": cleaned_reviews})\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:43.340221Z","iopub.execute_input":"2024-01-10T11:11:43.340704Z","iopub.status.idle":"2024-01-10T11:11:43.353159Z","shell.execute_reply.started":"2024-01-10T11:11:43.340663Z","shell.execute_reply":"2024-01-10T11:11:43.351881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install --upgrade nltk","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:43.356086Z","iopub.execute_input":"2024-01-10T11:11:43.356871Z","iopub.status.idle":"2024-01-10T11:11:43.373946Z","shell.execute_reply.started":"2024-01-10T11:11:43.356836Z","shell.execute_reply":"2024-01-10T11:11:43.372416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train set\n\nstart_time = time.time()\ntrain_data = split_and_clean_reviews(X_train)\nelapsed_time = time.time() - start_time\n\nprint(f\"The time difference is: {elapsed_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:43.37743Z","iopub.execute_input":"2024-01-10T11:11:43.378196Z","iopub.status.idle":"2024-01-10T11:11:44.836774Z","shell.execute_reply.started":"2024-01-10T11:11:43.37815Z","shell.execute_reply":"2024-01-10T11:11:44.835503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test set\n\nstart_time = time.time()\ntest_data = split_and_clean_reviews(X_test)\nelapsed_time = time.time() - start_time\n\nprint(f\"The time difference is: {elapsed_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:44.838736Z","iopub.execute_input":"2024-01-10T11:11:44.83953Z","iopub.status.idle":"2024-01-10T11:11:45.191304Z","shell.execute_reply.started":"2024-01-10T11:11:44.839498Z","shell.execute_reply":"2024-01-10T11:11:45.190188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:45.193207Z","iopub.execute_input":"2024-01-10T11:11:45.194221Z","iopub.status.idle":"2024-01-10T11:11:45.207317Z","shell.execute_reply.started":"2024-01-10T11:11:45.194184Z","shell.execute_reply":"2024-01-10T11:11:45.206289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:45.208692Z","iopub.execute_input":"2024-01-10T11:11:45.209232Z","iopub.status.idle":"2024-01-10T11:11:45.224371Z","shell.execute_reply.started":"2024-01-10T11:11:45.209194Z","shell.execute_reply":"2024-01-10T11:11:45.222788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Aspect extraction -  process of identifying and extracting specific aspects, features, or attributes from textual data","metadata":{}},{"cell_type":"code","source":"def apply_extraction(row, nlp):\n    \"\"\"\n    This function extracts aspect and its corresponding description from the review by \n    applying 7 different rules of POS tagging.\n    \n    Args:\n        row (pd.Series): A row from a DataFrame containing the 'Review' column.\n        nlp (spacy.Language): The spaCy NLP pipeline.\n\n    Returns:\n        dict: A dictionary containing the extracted aspect pairs.\n        \n    \"\"\"\n    prod_pronouns = ['it', 'this', 'they', 'these']\n    review_body = row['Review']\n    doc = nlp(review_body)\n    \n    aspect_pairs = []\n    \n    for token in doc:\n        # Rule 1\n        aspect_pairs.extend(rule1(token, prod_pronouns))\n        \n        # Rule 2\n        aspect_pairs.extend(rule2(token, prod_pronouns))\n        \n        # Rule 3\n        aspect_pairs.extend(rule3(token, prod_pronouns))\n        \n        # Rule 4\n        aspect_pairs.extend(rule4(token, prod_pronouns))\n        \n        # Rule 5\n        aspect_pairs.extend(rule5(token, prod_pronouns))\n        \n        # Rule 6\n        aspect_pairs.extend(rule6(token, prod_pronouns))\n        \n        # Rule 7\n        aspect_pairs.extend(rule7(token, prod_pronouns))\n        \n    return {\"aspect_pairs\": aspect_pairs}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The rules below extract aspects (A) and sentiment modifiers (M) based on the specified dependency relationships","metadata":{}},{"cell_type":"markdown","source":"Certainly! Let's include information about the head of the children for each token in the new sentence \"She quickly ate the delicious cake.\"\n\n**A. Dependency Parsing Concepts:**\n\nLet's simplify it even further:\n\nAlright, let's simplify it further:\n\n**1. Child:it's connected to another word -> Each word has a specific role, and it's connected to another word that guides or influences it nsubj, advmod, dobj;**\n- Think of words in a sentence like members of a team. Each word has a specific role, and it's connected to another word that guides or influences it.\n- The connection between words has a special label that tells us the job or role of each word in relation to the other.\n\n**2. Head of a Token: BOOS determines what role each word plays in the sentence**\n- Imagine words in a sentence as a group, and each word has a leader. This leader (head) determines what role each word plays in the sentence.\n- The leader word is like the boss, directing others and making sure everything fits together properly.\n\nIn short, a child is a team member with a specific role, connected to another word that guides it. The head is the leader that directs and organizes the roles of all the words in the sentence.\n\nSo, in simpler terms, a child is like a family member with a specific role, and the head is like the boss, guiding and influencing the roles of others in the sentence.\n\n3. **Head of Children:**\n   - Refers to the word that governs the grammatical relationship with the children of a token.\n\n**B. Example Application on \"She quickly ate the delicious cake\":**\n\n1. **\"She\":**\n   - Child: None (no dependents)\n   - Head: \"ate\"\n   - Head of Children: None (no children)\n\n2. **\"Ate\":**\n   - Children: \"She\" (nsubj), \"quickly\" (advmod), \"cake\" (dobj)\n   - Head: None (root of the dependency tree)\n   - Head of Children: \"She\" (nsubj), \"quickly\" (advmod), \"cake\" (dobj)\n\n3. **\"Quickly\":**\n   - Child: None (no dependents)\n   - Head: \"ate\"\n   - Head of Children: None (no children)\n\n4. **\"The\":**\n   - Child: None (no dependents)\n   - Head: \"cake\"\n   - Head of Children: None (no children)\n\n5. **\"Delicious\":**\n   - Child: None (no dependents)\n   - Head: \"cake\"\n   - Head of Children: None (no children)\n\n6. **\"Cake\":**\n   - Children: \"The\" (det), \"delicious\" (amod)\n   - Head: \"ate\"\n   - Head of Children: \"The\" (det), \"delicious\" (amod)\n\n**Summary of Relationships:**\n- \"She\" has \"ate\" as its head. No children.\n- \"Ate\" has \"She\" (nsubj), \"quickly\" (advmod), and \"cake\" (dobj) as its children and has no head (it is the root). The head of children is \"She\" (nsubj), \"quickly\" (advmod), \"cake\" (dobj).\n- \"Quickly\" has \"ate\" as its head. No children.\n- \"The\" and \"delicious\" both have \"cake\" as their head. No children.\n- \"Cake\" has \"The\" (det) and \"delicious\" (amod) as its children and \"ate\" as its head. The head of children is \"The\" (det), \"delicious\" (amod).\n\nThis breakdown provides a more detailed view of the relationships between each token, its head, and the children of each token, including the head of the children.","metadata":{}},{"cell_type":"code","source":"def rule1(token, prod_pronouns):\n    \"\"\"\n    Apply Rule 1: Extract aspect and its corresponding description from the review.\n    \n    **Adverbial Modifier of Adjective Rule :**\n\n    This rule focuses on extracting aspects (A) and their corresponding sentiment modifiers (M) from a sentence. \n    It specifically looks for adjectival modifiers (`amod`) in the dependency relations of a token, excluding stop words. \n    The aspect (A) is identified as the head token's text, and the sentiment modifier (M) is updated with the current token's text. \n    Additionally, it considers adverbial modifiers of adjectives and handles negation in adjectives. \n    The result is formatted, and if the aspect is one of the specified pronouns, it is replaced with \"product.\"\n        \n    Args:\n        token (spacy.Token): The input token.\n        prod_pronouns (list): List of pronouns to be replaced with \"product.\"\n\n    Returns:\n        list: A list containing a dictionary with the extracted aspect, sentiment modifier, and rule number (1).\n        \n    \"\"\"\n    A, M = \"999999\", \"999999\" # A - the aspect or feature being described in the sentence, in the phrase \"sound quality,\" \"sound\" would be the aspect. M - Sentiment Modifier In the phrase \"good sound quality,\" \"good\" would be the sentiment modifier.\n    \n    if token.dep_ == \"amod\" and not token.is_stop: # checks if the token has a dependency relation of \"amod\" (adjectival modifier) and is not a stop word.\n        M = token.text # it updates M with the current token's text \n        A = token.head.text # and A with the head token's text - the aspect or feature being described in the sentence\n        \n        # add adverbial modifier of adjective (e.g. 'most comfortable headphones')\n        M_children = [child_m.text for child_m in token.children if child_m.dep_ == \"advmod\"]\n        if M_children:\n            M = \" \".join([M] + M_children)\n            \n        # negation in adjective, the \"no\" keyword is a determiners of the noun (e.g., no interesting characters) ; Determiners include articles (a, an, the), demonstratives (this, that, these, those), possessive pronouns (my, your, his, her, its, our, their), and other words that provide information about the noun.\n        A_children = [child_a for child_a in token.head.children if child_a.dep_ == \"det\" and child_a.text == 'no']\n        if A_children:\n            neg_prefix = 'not'\n            M = f\"{neg_prefix} {M}\"\n\n    if A != \"999999\" and M != \"999999\":\n        if A in prod_pronouns:\n            A = \"product\"\n        return [{\"noun\": A, \"adj\": M, \"rule\": 1}]\n    return []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rule2(token):\n    \"\"\"\n    Apply Rule 2: Extract aspect and its corresponding description from the review.\n    \n    **Direct Object Rule:**\n    \n    This rule extracts aspects (A) and sentiment modifiers (M) from a sentence by focusing on the relationship \n    between the nominal subject (A) and the direct object (M) of a verb.\n    It assumes that a verb has only one nominal subject and one direct object.\n    \n    Args:\n        token (spacy.Token): The input token.\n\n    Returns:\n        list: A list containing a dictionary with the extracted aspect, sentiment modifier, and rule number (2).\n    \"\"\"\n    A, M = \"999999\", \"999999\"\n    add_neg_pfx = False\n\n    # Iterate through the children of the token\n    for child in token.children:\n        # If the child is the nominal subject (nsubj) and not a stop word\n        if child.dep_ == \"nsubj\" and not child.is_stop:\n            A = child.text\n\n        # If the child is the direct object (dobj) and its part-of-speech is ADJ (adjective), and not a stop word\n        if child.dep_ == \"dobj\" and child.pos_ == \"ADJ\" and not child.is_stop:\n            M = child.text\n    \n            # If the child is a negation word\n        if child.dep_ == \"neg\":\n            neg_prefix = child.text\n            add_neg_pfx = True\n            \n        # If negation is present, add the negation prefix to the sentiment modifier (M)\n        if add_neg_pfx and M != \"999999\":\n            M = f\"{neg_prefix} {M}\"\n        \n        # If both aspect (A) and sentiment modifier (M) are valid, format the result\n        if A != \"999999\" and M != \"999999\":\n            # If aspect is one of the specified pronouns, replace it with \"product\"\n            if A in prod_pronouns:\n                A = \"product\"\n            return [{\"noun\": A, \"adj\": M, \"rule\": 2}]\n\n        # If no valid aspect-sentiment pair is found, return an empty list\n        return []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rule3(token, prod_pronouns):\n    \"\"\"\n    Apply Rule 3: Extract aspect and its corresponding description from the review.\n    \n    **Adjectival Complement Rule:**\n\n    This rule identifies aspects (A) and sentiment modifiers (M) by examining the relationship between the nominal subject (A) \n    and the adjectival complement (M) in a sentence. It assumes a verb has a single nominal subject and considers the adjectival complement as \n    the sentiment modifier. The presence \n    of an auxiliary (AUX) dependency helps handle cases like \"The sound of the speakers would be better\" or \"The sound of the speakers could be better.\"\n    \n    Args:\n        token (spacy.tokens.Token): The token to analyze.\n        prod_pronouns (list): List of product pronouns.\n\n    Returns:\n        list: A list of dictionaries containing aspect, sentiment modifier, and rule number (3).\n    \"\"\"\n    A, M = \"999999\", \"999999\"\n    add_neg_pfx = False\n\n    for child in token.children:\n        # Check if the child is the nominal subject (nsubj) and not a stop word\n        if child.dep_ == \"nsubj\" and not child.is_stop:\n            A = child.text\n\n        # Check if the child is the adjectival complement (acomp) and not a stop word\n        if child.dep_ == \"acomp\" and not child.is_stop:\n            M = child.text\n\n        # Example: 'this could have been better' -> (this, not better)\n        # If a child is an auxiliary (aux) with the tag \"MD\"  - Modal Auxiliary or a negation word (MD = \"can,\" \"could,\" \"will,\" \"would,\" \"shall,\" \"should,\" \"may,\" \"might,\" and \"must.\")\n        if child.dep_ == \"aux\" and child.tag_ == \"MD\":\n            neg_prefix = \"not\"\n            add_neg_pfx = True\n\n        # Check if the child is a negation word\n        if child.dep_ == \"neg\":\n            neg_prefix = child.text\n            add_neg_pfx = True\n\n    # If negation is present, add the negation prefix to the sentiment modifier (M)\n    if add_neg_pfx and M != \"999999\":\n        M = f\"{neg_prefix} {M}\"\n\n    # If both aspect (A) and sentiment modifier (M) are valid, format the result\n    if A != \"999999\" and M != \"999999\":\n        # If aspect is one of the specified pronouns, replace it with \"product\"\n        if A in prod_pronouns:\n            A = \"product\"\n        return [{\"noun\": A, \"adj\": M, \"rule\": 3}]\n\n    # If no valid aspect-sentiment pair is found, return an empty list\n    return []\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rule4(token):\n    \"\"\"\n    Apply Rule 4: Extract aspect and its corresponding description from the review.\n    \n    **Passive Adverbial Modifier Rule:**\n\n    This rule focuses on identifying aspects (A) and sentiment modifiers (M) by examining the relationship between the nominal subject (A) and \n    the adverbial modifier (M) in a sentence where\n    the verb is in the passive voice (nsubjpass) or nsubj. It assumes a verb has a single nominal subject, and the adverbial modifier serves as the sentiment modifier.\n    \n    * nominal subject (nsubj) or passive nominal subject (nsubjpass)\n    \n    Args:\n    - token (spacy.Token): A token from a processed Spacy document.\n\n    Returns:\n    - list: A list containing dictionaries with extracted aspect-sentiment pairs based on Rule 4. Each dictionary has keys 'noun', 'adj', and 'rule'.\n            If no valid aspect-sentiment pair is found, an empty list is returned.\n    \"\"\"\n    \n    \n    A, M = \"999999\", \"999999\"\n    add_neg_pfx = False\n    children = token.children\n\n    # Iterate through the children of the token\n    for child in children:\n        # If the child is the nominal subject (nsubjpass or nsubj) and not a stop word\n        if (child.dep_ == \"nsubjpass\" or child.dep_ == \"nsubj\") and not child.is_stop:\n            A = child.text\n\n        # If the child is an adverbial modifier (advmod) and not a stop word\n        if child.dep_ == \"advmod\" and not child.is_stop:\n            M = child.text\n            \n            # Check for additional adverbial modifier of the main advmod\n            M_children = child.children\n            for child_m in M_children:\n                if child_m.dep_ == \"advmod\":\n                    M_hash = child_m.text\n                    M = M_hash + \" \" + child.text\n                    break\n\n        # If the child is a negation word\n        if child.dep_ == \"neg\":\n            neg_prefix = child.text\n            add_neg_pfx = True\n\n    # If negation is present, add the negation prefix to the sentiment modifier (M)\n    if add_neg_pfx and M != \"999999\":\n        M = neg_prefix + \" \" + M\n\n    # If both aspect (A) and sentiment modifier (M) are valid, format the result\n    if A != \"999999\" and M != \"999999\":\n        # If aspect is one of the specified pronouns, replace it with \"product\"\n        if A in ['it', 'this', 'they', 'these']:\n            A = \"product\"\n        return [{\"noun\": A, \"adj\": M, \"rule\": 4}]\n\n    # If no valid aspect-sentiment pair is found, return an empty list\n    return []\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rule5(token):\n    \"\"\"\n    Apply Rule 5: Extract aspect and its corresponding description from the review.\n    \n    **Copular Verb Complement Rule:**\n\n    This rule identifies aspects (A) and sentiment modifiers (M) by examining the relationship between the nominal subject (A) and the complement (M)\n    of a copular verb.\n    In this context, it assumes that a verb has only one nominal subject, and the complement serves as the sentiment modifier.\n        \n    For example, in the sentence \"The product is durable,\" the copula is \"is,\" and the nominal subject is \"The product.\n\n    Args:\n    - token (spacy.Token): A token from a processed Spacy document.\n\n    Returns:\n    - list: A list containing dictionaries with extracted aspect-sentiment pairs based on Rule 5. \n            Each dictionary has keys 'noun', 'adj', and 'rule'.\n            If no valid aspect-sentiment pair is found, an empty list is returned.\n    \"\"\"\n    A, buf_var = \"999999\", \"999999\"\n    children = token.children\n\n    # Iterate through the children of the token\n    for child in children:\n        # If the child is the nominal subject (nsubj) and not a stop word\n        if child.dep_ == \"nsubj\" and not child.is_stop:\n            A = child.text\n\n        # If the child is the copula (cop) and not a stop word\n        if child.dep_ == \"cop\" and not child.is_stop:\n            buf_var = child.text\n\n    # If both aspect (A) and copula (buf_var) are valid, format the result\n    if A != \"999999\" and buf_var != \"999999\":\n        # If aspect is one of the specified pronouns, replace it with \"product\"\n        if A in ['it', 'this', 'they', 'these']:\n            A = \"product\"\n        return [{\"noun\": A, \"adj\": token.text, \"rule\": 5}]\n\n    # If no valid aspect-sentiment pair is found, return an empty list\n    return []\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rule6(token):\n    \"\"\"\n    Apply Rule 6: Extract aspect and its corresponding description from the review.\n    \n    **Interjection Rule:**\n\n    This rule focuses on extracting aspects (A) and sentiment modifiers (M) from interjections. \n    In examples like \"It's ok,\" where \"ok\" is classified as an interjection (INTJ), it considers the interjection as the sentiment modifier.\n\n    Args:\n        token (spacy.Token): The token to analyze.\n\n    Returns:\n        list: A list containing a dictionary with the extracted aspect, sentiment modifier, and rule number (6).\n    \"\"\"\n    A, M = \"999999\", \"999999\"\n\n    # Check if the token is an interjection (INTJ) and not a stop word\n    if token.pos_ == \"INTJ\" and not token.is_stop:\n        children = token.children\n        for child in children:\n            # If the child is the nominal subject (nsubj) and not a stop word\n            if child.dep_ == \"nsubj\" and not child.is_stop:\n                A = child.text\n                M = token.text\n\n    # If both aspect (A) and sentiment modifier (M) are valid, format the result\n    if A != \"999999\" and M != \"999999\":\n        # If aspect is one of the specified pronouns, replace it with \"product\"\n        if A in ['it', 'this', 'they', 'these']:\n            A = \"product\"\n        return [{\"noun\": A, \"adj\": M, \"rule\": 6}]\n\n    # If no valid aspect-sentiment pair is found, return an empty list\n    return []\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rule7(token):\n    \"\"\"\n    Apply Rule 7: Extract aspect and its corresponding description from the review.\n    \n    **Complement Rule:**\n\n    This rule identifies aspects (A) and sentiment modifiers (M) by looking at the link between a verb like 'be/seem/appear' and its complement \n    using the `ATTR` relationship. For example, in the sentence 'this is garbage,' it extracts the aspect 'this' and the sentiment modifier 'garbage.'\n\n    Args:\n        token (spacy.Token): The token to analyze.\n\n    Returns:\n        list: A list containing a dictionary with the extracted aspect, sentiment modifier, and rule number (7).\n    \"\"\"\n    A, M = \"999999\", \"999999\"\n    add_neg_pfx = False\n    children = token.children\n\n    # Iterate through the children of the token\n    for child in children:\n        # If the child is the nominal subject (nsubj) and not a stop word\n        if child.dep_ == \"nsubj\" and not child.is_stop:\n            A = child.text\n\n        # If the child is the attribute (attr) and not a stop word\n        if child.dep_ == \"attr\" and not child.is_stop:\n            M = child.text\n\n        # If the child is a negation word\n        if child.dep_ == \"neg\":\n            neg_prefix = child.text\n            add_neg_pfx = True\n\n    # If negation is present, add the negation prefix to the sentiment modifier (M)\n    if add_neg_pfx and M != \"999999\":\n        M = neg_prefix + \" \" + M\n\n    # If both aspect (A) and sentiment modifier (M) are valid, format the result\n    if A != \"999999\" and M != \"999999\":\n        # If aspect is one of the specified pronouns, replace it with \"product\"\n        if A in ['it', 'this', 'they', 'these']:\n            A = \"product\"\n        return [{\"noun\": A, \"adj\": M, \"rule\": 7}]\n\n    # If no valid aspect-sentiment pair is found, return an empty list\n    return []\n","metadata":{},"execution_count":null,"outputs":[]}]}