{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc-autonumbering":true,"toc-showcode":false,"toc-showmarkdowntxt":false,"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7375495,"sourceType":"datasetVersion","datasetId":4285675}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/diaconumadalina/summarization-reviews?scriptVersionId=158474899\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## 2 Setup","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Import and configure libraries ","metadata":{}},{"cell_type":"code","source":"# Data manipulation libraries\nimport pandas as pd\nimport spacy\nfrom spacy import displacy # is used for visualizing the dependency parse tree and named entity recognition (NER) annotations.\n\n# General Imports\nimport time\n\n# Data modeling libraries\nfrom sklearn.model_selection import train_test_split\n\n# text processing and cleaning\nimport re # This line imports the regular expression (regex) module, which provides functions for working with regular expressions. \nimport nltk\nfrom nltk.stem import WordNetLemmatizer #  is used for lemmatization, which is the process of reducing words to their base or root form: from running to run\nfrom nltk.corpus import stopwords #The stopwords corpus from NLTK contains common words that are often removed from text during text preprocessing/ These words (like 'and', 'the', 'is', etc.) are considered as noise in many natural language processing tasks \n\n# Display in jupyter\n# from IPython.core.display import display, HTML\n# # Set the width of the output cell\n# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:25.323843Z","iopub.execute_input":"2024-01-10T11:11:25.324336Z","iopub.status.idle":"2024-01-10T11:11:25.331914Z","shell.execute_reply.started":"2024-01-10T11:11:25.324302Z","shell.execute_reply":"2024-01-10T11:11:25.330678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Constants and helper functions ","metadata":{}},{"cell_type":"markdown","source":"#### Constants","metadata":{}},{"cell_type":"markdown","source":"#### Helper functions","metadata":{}},{"cell_type":"code","source":"def load_dataset_from_json(json_file_path):\n    \"\"\"\n    :param json_file_path (str) :Path to the JSON file.\n    :return: pd.DataFrame: DataFrame containing the loaded data.\n    \"\"\"\n    df = pd.read_json(json_file_path)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:25.334182Z","iopub.execute_input":"2024-01-10T11:11:25.33475Z","iopub.status.idle":"2024-01-10T11:11:25.353928Z","shell.execute_reply.started":"2024-01-10T11:11:25.334692Z","shell.execute_reply":"2024-01-10T11:11:25.352718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Set dataframe ","metadata":{}},{"cell_type":"code","source":"df = load_dataset_from_json(\"/kaggle/input/amazon-one-plus-reviews/amazon_one_plus_reviews.json\")\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:25.356126Z","iopub.execute_input":"2024-01-10T11:11:25.358121Z","iopub.status.idle":"2024-01-10T11:11:26.261191Z","shell.execute_reply.started":"2024-01-10T11:11:25.35807Z","shell.execute_reply":"2024-01-10T11:11:26.259835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# conda update --all","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:26.26284Z","iopub.execute_input":"2024-01-10T11:11:26.263223Z","iopub.status.idle":"2024-01-10T11:11:26.268875Z","shell.execute_reply.started":"2024-01-10T11:11:26.26319Z","shell.execute_reply":"2024-01-10T11:11:26.26768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:26.271664Z","iopub.execute_input":"2024-01-10T11:11:26.27217Z","iopub.status.idle":"2024-01-10T11:11:26.370752Z","shell.execute_reply.started":"2024-01-10T11:11:26.272136Z","shell.execute_reply":"2024-01-10T11:11:26.369488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:26.372008Z","iopub.execute_input":"2024-01-10T11:11:26.372358Z","iopub.status.idle":"2024-01-10T11:11:28.354361Z","shell.execute_reply.started":"2024-01-10T11:11:26.372328Z","shell.execute_reply":"2024-01-10T11:11:28.353082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The dataset from worlbank contains information about 3 types of produc in our application we need for startjust one product\ndf['product'].value_counts()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-01-10T11:11:28.356443Z","iopub.execute_input":"2024-01-10T11:11:28.357172Z","iopub.status.idle":"2024-01-10T11:11:28.371735Z","shell.execute_reply.started":"2024-01-10T11:11:28.357138Z","shell.execute_reply":"2024-01-10T11:11:28.370777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = df['product'] == 'Redmi Note 8 (Neptune Blue, 4GB RAM, 64GB Storage)'\ndf = df[mask].reset_index(drop=True)\ndf = df[['reviewed_at', 'review_text', 'review_title']]\ndf = df.rename(columns = {'review_title' : 'Summary', 'review_text' : 'Review', 'reviewed_at' : 'Date'})","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.373055Z","iopub.execute_input":"2024-01-10T11:11:28.373418Z","iopub.status.idle":"2024-01-10T11:11:28.410985Z","shell.execute_reply.started":"2024-01-10T11:11:28.373387Z","shell.execute_reply":"2024-01-10T11:11:28.409755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Format the date as \"30 August 2021\"\ndf['Date'] = df['Date'].apply(lambda x: x.strftime('%d %B %Y') if not pd.isnull(x) else '')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.412291Z","iopub.execute_input":"2024-01-10T11:11:28.412681Z","iopub.status.idle":"2024-01-10T11:11:28.552316Z","shell.execute_reply.started":"2024-01-10T11:11:28.412649Z","shell.execute_reply":"2024-01-10T11:11:28.55105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3 EDA","metadata":{}},{"cell_type":"markdown","source":"##### For this analyse I will use the product named Redmi Note 8 (Neptune Blue, 4GB RAM, 64GB Storage) 13934\n","metadata":{}},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.558047Z","iopub.execute_input":"2024-01-10T11:11:28.558492Z","iopub.status.idle":"2024-01-10T11:11:28.570089Z","shell.execute_reply.started":"2024-01-10T11:11:28.558457Z","shell.execute_reply":"2024-01-10T11:11:28.569042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.571813Z","iopub.execute_input":"2024-01-10T11:11:28.572489Z","iopub.status.idle":"2024-01-10T11:11:28.598059Z","shell.execute_reply.started":"2024-01-10T11:11:28.572453Z","shell.execute_reply":"2024-01-10T11:11:28.596859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4 Training Setup","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Train, Test split","metadata":{}},{"cell_type":"markdown","source":"##### Train, Test split","metadata":{}},{"cell_type":"code","source":"X = df.drop([\"Summary\"], axis =1 )\nX_train, X_test = train_test_split(X , test_size = 0.2, random_state = 0)\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.600152Z","iopub.execute_input":"2024-01-10T11:11:28.601266Z","iopub.status.idle":"2024-01-10T11:11:28.623204Z","shell.execute_reply.started":"2024-01-10T11:11:28.601226Z","shell.execute_reply":"2024-01-10T11:11:28.622007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Sentence Tokenization\n","metadata":{}},{"cell_type":"code","source":"def split_review_custom_delimiters(text):\n    \"\"\"\n    This function splits the review into multiple sentences based on custom delimiters.\n    \n    Args:\n        text (str): The input text to be split.\n    Returns:\n        list: A list of sentences after splitting based on the specified custom delimiters.\n    \"\"\"\n    delimiters = \".\", \"but\", \"and\", \"also\"\n    escaped_delimiters = map(re.escape, delimiters) # Result: ['\\\\.', 'but', 'and', 'also']\n    regex_pattern = '|'.join(escaped_delimiters) # Applying the custom delimiters # Result: '\\\\.|but|and|also'\n    splitted = re.split(regex_pattern, text) # Splitting the review function from the re module to split the input text into a list of substrings based on the specified regular expression pattern.\n    return[sentence.strip() for sentence in splitted if sentence.strip()] #this line ensures that only non-empty sentences (after stripping whitespaces) are included in the final result.  sentence.strip(): Strips any leading or trailing whitespaces from the sentence.","metadata":{"execution":{"iopub.status.busy":"2024-01-10T17:10:27.078719Z","iopub.execute_input":"2024-01-10T17:10:27.079175Z","iopub.status.idle":"2024-01-10T17:10:27.086704Z","shell.execute_reply.started":"2024-01-10T17:10:27.079138Z","shell.execute_reply":"2024-01-10T17:10:27.085539Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"##### Data cleaning","metadata":{}},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.639077Z","iopub.execute_input":"2024-01-10T11:11:28.639636Z","iopub.status.idle":"2024-01-10T11:11:28.658087Z","shell.execute_reply.started":"2024-01-10T11:11:28.639587Z","shell.execute_reply":"2024-01-10T11:11:28.657069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemma = WordNetLemmatizer()\nall_stopwords = set(stopwords.words('english'))\n\ncustom_stopwords = ['not', 'but', 'because', 'against', 'between', 'up', 'down', 'in', 'out', 'once', 'before','after', 'few', 'more', 'most', 'no', 'nor', 'same', 'some']\n\nfor stopword in custom_stopwords:\n    all_stopwords.remove(stopword)\n\ndef clean_aspect_spacy(reviews):\n    \"\"\"\n    this function prepares text for analysis by cleaning it, making it more consistent, \n    and removing elements that may not carry substantial meaning for downstream tasks in natural language processing as punctuations, stopwords, and other non-alphanumeric characters.\n    It expands contractions and replaces some words with an empty string.\n    \n    Args:\n        reviews (str): The text to be cleaned.\n        lemma (WordNetLemmatizer): An instance of WordNetLemmatizer for lemmatization.\n        all_stopwords (set): A set of stopwords to be removed from the text.\n\n    Returns:\n        str: The cleaned and preprocessed text.\n        \n    \"\"\"\n    text = reviews.lower()\n    \n    contractions = {\n        \"won't\": \"will not\",\n        \"cannot\": \"can not\",\n        \"can't\": \"can not\",\n        \"n't\": \" not\",\n        \"what's\": \"what is\",\n        \"it's\": \"it is\",\n        \"'ve\": \" have\",\n        \"i'm\": \"i am\",\n        \"'re\": \" are\",\n        \"he's\": \"he is\",\n        \"she's\": \"she is\",\n        \"*****\": \" \",        \n    }\n    \n    for contraction, expansion in contractions.items():\n        text = text.replace(contraction, expansion)\n        \n    # Remove special characters, numbers, and extra spaces.\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n    text = re.sub(' +', ' ', text)\n    \n    # Lemmatization and removing stopwords\n    words = text.split()\n    cleaned_words = [lemma.lemmatize(word) for word in words if word not in set(all_stopwords)]\n    \n    # Join the cleaned words back into a sentence\n    cleaned_text = ' '.join(cleaned_words) \n    \n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.659468Z","iopub.execute_input":"2024-01-10T11:11:28.660654Z","iopub.status.idle":"2024-01-10T11:11:28.675378Z","shell.execute_reply.started":"2024-01-10T11:11:28.660616Z","shell.execute_reply":"2024-01-10T11:11:28.673798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Reconstruct the DataFrame","metadata":{}},{"cell_type":"code","source":"nltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:28.677378Z","iopub.execute_input":"2024-01-10T11:11:28.678618Z","iopub.status.idle":"2024-01-10T11:11:28.69536Z","shell.execute_reply.started":"2024-01-10T11:11:28.678565Z","shell.execute_reply":"2024-01-10T11:11:28.693686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install --upgrade nltk","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:46.386582Z","iopub.execute_input":"2024-01-10T11:11:46.386986Z","iopub.status.idle":"2024-01-10T11:11:46.392356Z","shell.execute_reply.started":"2024-01-10T11:11:46.386956Z","shell.execute_reply":"2024-01-10T11:11:46.391063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_and_clean_reviews(df):\n    \"\"\"\n    This function takes a DataFrame 'df' with 'Review' and 'Date' columns, splits each review into smaller components,\n    filters out components with fewer than three words, and applies a text cleaning function to each split and cleaned review.\n    \n    Parameters:\n    - df (DataFrame): Input DataFrame with 'Review' and 'Date' columns.\n\n    Returns:\n    DataFrame: A new DataFrame with 'Date' and 'Review' columns, where each review has been split, filtered, and cleaned.\n    \"\"\"\n    reviews = []\n    dates = []\n    \n    for i, review_text in enumerate(df[\"Review\"].values):\n        review_split = split_review_custom_delimiters(review_text)\n        \n        # Filter out components with fewer than three words\n        review_split_filtered = [split for split in review_split if len(split.split()) >= 3]\n    \n        # Duplicate dates as string for the corresponding split reviews\n        duplicate_dates = [str(df[\"Date\"].values[i]) for _ in range(len(review_split_filtered))]\n\n        # Extend the lists with split and duplicated reviews and dates\n        reviews.extend(review_split_filtered)\n        dates.extend(duplicate_dates)\n\n    # Apply the text cleaning function to each split and cleaned review    \n    cleaned_reviews = [clean_aspect_spacy(text) for text in reviews]\n    \n    # Create a new DataFrame with 'Date' and 'Review' columns\n    data = pd.DataFrame({\"Date\": dates, \"Review\": cleaned_reviews})\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:43.340221Z","iopub.execute_input":"2024-01-10T11:11:43.340704Z","iopub.status.idle":"2024-01-10T11:11:43.353159Z","shell.execute_reply.started":"2024-01-10T11:11:43.340663Z","shell.execute_reply":"2024-01-10T11:11:43.351881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install --upgrade nltk","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:43.356086Z","iopub.execute_input":"2024-01-10T11:11:43.356871Z","iopub.status.idle":"2024-01-10T11:11:43.373946Z","shell.execute_reply.started":"2024-01-10T11:11:43.356836Z","shell.execute_reply":"2024-01-10T11:11:43.372416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train set\n\nstart_time = time.time()\ntrain_data = split_and_clean_reviews(X_train)\nelapsed_time = time.time() - start_time\n\nprint(f\"The time difference is: {elapsed_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:43.37743Z","iopub.execute_input":"2024-01-10T11:11:43.378196Z","iopub.status.idle":"2024-01-10T11:11:44.836774Z","shell.execute_reply.started":"2024-01-10T11:11:43.37815Z","shell.execute_reply":"2024-01-10T11:11:44.835503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test set\n\nstart_time = time.time()\ntest_data = split_and_clean_reviews(X_test)\nelapsed_time = time.time() - start_time\n\nprint(f\"The time difference is: {elapsed_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:44.838736Z","iopub.execute_input":"2024-01-10T11:11:44.83953Z","iopub.status.idle":"2024-01-10T11:11:45.191304Z","shell.execute_reply.started":"2024-01-10T11:11:44.839498Z","shell.execute_reply":"2024-01-10T11:11:45.190188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:45.193207Z","iopub.execute_input":"2024-01-10T11:11:45.194221Z","iopub.status.idle":"2024-01-10T11:11:45.207317Z","shell.execute_reply.started":"2024-01-10T11:11:45.194184Z","shell.execute_reply":"2024-01-10T11:11:45.206289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T11:11:45.208692Z","iopub.execute_input":"2024-01-10T11:11:45.209232Z","iopub.status.idle":"2024-01-10T11:11:45.224371Z","shell.execute_reply.started":"2024-01-10T11:11:45.209194Z","shell.execute_reply":"2024-01-10T11:11:45.222788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Aspect extraction -  process of identifying and extracting specific aspects, features, or attributes from textual data","metadata":{}},{"cell_type":"code","source":"def apply_extraction(row, nlp):\n    \"\"\"\n    This function extracts aspect and its corresponding description from the review by \n    applying 7 different rules of POS tagging.\n    \n    Args:\n        row (pd.Series): A row from a DataFrame containing the 'Review' column.\n        nlp (spacy.Language): The spaCy NLP pipeline.\n\n    Returns:\n        dict: A dictionary containing the extracted aspect pairs.\n        \n    \"\"\"\n    prod_pronouns = ['it', 'this', 'they', 'these']\n    review_body = row['Review']\n    doc = nlp(review_body)\n    \n    aspect_pairs = []\n    \n    for token in doc:\n        # Rule 1\n        aspect_pairs.extend(rule1(token, prod_pronouns))\n        \n        # Rule 2\n        aspect_pairs.extend(rule2(token, prod_pronouns))\n        \n        # Rule 3\n        aspect_pairs.extend(rule3(token, prod_pronouns))\n        \n        # Rule 4\n        aspect_pairs.extend(rule4(token, prod_pronouns))\n        \n        # Rule 5\n        aspect_pairs.extend(rule5(token, prod_pronouns))\n        \n        # Rule 6\n        aspect_pairs.extend(rule6(token, prod_pronouns))\n        \n        # Rule 7\n        aspect_pairs.extend(rule7(token, prod_pronouns))\n        \n    return {\"aspect_pairs\": aspect_pairs}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The rules below extract aspects (A) and sentiment modifiers (M) based on the specified dependency relationships","metadata":{}},{"cell_type":"markdown","source":"**Child**: In the syntactic structure of a sentence, a \"child\" refers to a word that is directly connected to another word (the \"parent\") in a hierarchical tree structure. The relationship between a child and its parent is defined by a specific dependency label, such as nsubj (nominal subject), acomp (adjectival complement), aux (auxiliary), etc. Children are elements that depend on another word to form a grammatically correct sentence.\n\nThe term \"**head of a token**\" in the context of dependency parsing refers to the word that governs the grammatical relationship with the token in question. Every word in a sentence has a syntactic relationship with another word, and the head of a token is the word that determines this relationship.\n\n**A In the given sentence \"The cat chased the mouse,\" let's break down the relationships between each word and its children and head in a simple dependency structure:**\n\n1. **The:**\n   - Child: None (it has no dependents)\n   - Head: cat\n\n2. **cat:**\n   - Child: The (det)\n   - Head: chased\n\n3. **chased:**\n   - Children: cat (nsubj), mouse (dobj)\n   - Head: None (it is the root of the dependency tree)\n\n4. **the:**\n   - Child: None (it has no dependents)\n   - Head: mouse\n\n5. **mouse:**\n   - Child: the (det)\n   - Head: chased\n\nSo, in summary:\n\n- \"The\" has \"cat\" as its head.\n- \"Cat\" has \"The\" as its child and \"chased\" as its head.\n- \"Chased\" has \"cat\" and \"mouse\" as its children and has no head (it is the root).\n- \"The\" has \"mouse\" as its head.\n- \"Mouse\" has \"the\" as its child and \"chased\" as its head.\n\nIn this way, each token has relationships with other tokens, and the head of a token is the word that governs the grammatical relationship with that token. The children of a token are the words that depend on it in the sentence structure.\n\n\n**B In next example, each token is processed, and we print information about its children, head, and the children of its head.**\n\nTo provide examples of `token.children`, `token.head`, and `token.head.children`, I'll use the sentence \"The cat chased the mouse\" and assume that the sentence has been parsed into a dependency tree. Let's use a simple example where \"chased\" is the root of the dependency tree.\n\n```plaintext\nThe (det) cat (nsubj) chased (root) the (det) mouse (dobj)\n```\n\nIn this example:\n- `token` refers to each word in the sentence.\n- `token.children` refers to the immediate dependents of the token.\n- `token.head` refers to the token's syntactic parent in the dependency tree.\n- `token.head.children` refers to the immediate dependents of the token's syntactic parent.\n\nNow, let's go through the examples:\n\n1. For the word \"cat\":\n   - `token.children`: None (it has no dependents)\n   - `token.head`: chased\n   - `token.head.children`: The (det), chased (nsubj)\n\n2. For the word \"chased\":\n   - `token.children`: The (det), cat (nsubj), mouse (dobj)\n   - `token.head`: None (it is the root of the dependency tree)\n   - `token.head.children`: The (det), cat (nsubj), mouse (dobj)\n\n3. For the word \"mouse\":\n   - `token.children`: None (it has no dependents)\n   - `token.head`: chased\n   - `token.head.children`: the (det)\n","metadata":{}},{"cell_type":"code","source":"def rule1(token, prod_pronouns):\n    \"\"\"\n    Apply Rule 1: Extract aspect and its corresponding description from the review.\n    \n    A - the aspect or feature being described in the sentence. In the phrase \"sound quality,\" \"sound\" would be the aspect.\n    M - Sentiment Modifier. In the phrase \"good sound quality,\" \"good\" would be the sentiment modifier.\n    \n    This rule checks if the token has a dependency relation of \"amod\" (adjectival modifier) and is not a stop word.\n    It updates M with the current token's text and A with the head token's text - the aspect or feature being described in the sentence.\n    It also considers adverbial modifiers of adjectives and handles negation in adjectives.\n    \n    If both aspect (A) and sentiment modifier (M) are valid, it formats the result, and if A is one of the specified pronouns, it replaces it with \"product.\"\n    \n    Args:\n        token (spacy.Token): The input token.\n        prod_pronouns (list): List of pronouns to be replaced with \"product.\"\n\n    Returns:\n        list: A list containing a dictionary with the extracted aspect, sentiment modifier, and rule number (1).\n        \n    \"\"\"\n    A, M = \"999999\", \"999999\" # A - the aspect or feature being described in the sentence, in the phrase \"sound quality,\" \"sound\" would be the aspect. M - Sentiment Modifier In the phrase \"good sound quality,\" \"good\" would be the sentiment modifier.\n    \n    if token.dep_ == \"amod\" and not token.is_stop: # checks if the token has a dependency relation of \"amod\" (adjectival modifier) and is not a stop word.\n        M = token.text # it updates M with the current token's text \n        A = token.head.text # and A with the head token's text - the aspect or feature being described in the sentence\n        \n        # add adverbial modifier of adjective (e.g. 'most comfortable headphones')\n        M_children = [child_m.text for child_m in token.children if child_m.dep_ == \"advmod\"]\n        if M_children:\n            M = \" \".join([M] + M_children)\n            \n        # negation in adjective, the \"no\" keyword is a determiners of the noun (e.g., no interesting characters) ; Determiners include articles (a, an, the), demonstratives (this, that, these, those), possessive pronouns (my, your, his, her, its, our, their), and other words that provide information about the noun.\n        A_children = [child_a for child_a in token.head.children if child_a.dep_ == \"det\" and child_a.text == 'no']\n        if A_children:\n            neg_prefix = 'not'\n            M = f\"{neg_prefix} {M}\"\n\n    if A != \"999999\" and M != \"999999\":\n        if A in prod_pronouns:\n            A = \"product\"\n        return [{\"noun\": A, \"adj\": M, \"rule\": 1}]\n    return []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rule2(token):\n    \"\"\"\n    Apply Rule 2: Extract aspect and its corresponding description from the review.\n    \n    When analyzing the structure of a sentence, Rule 2 focuses on the relationship between the nominal subject (A) and the direct object (M) of a verb. In this context:\n    \n    - A is the aspect or feature being described, and it is identified as the nominal subject (nsubj).\n    - M is the sentiment modifier, and it is identified as the direct object (dobj) with an additional condition that its part-of-speech is an adjective (ADJ).\n\n    The rule assumes that a verb in the sentence will have only one nominal subject and one direct object, and it leverages this relationship to extract aspects and their corresponding sentiment modifiers.\n    \n    Args:\n        token (spacy.Token): The input token.\n\n    Returns:\n        list: A list containing a dictionary with the extracted aspect, sentiment modifier, and rule number (2).\n    \"\"\"\n    A, M = \"999999\", \"999999\"\n    add_neg_pfx = False\n\n    # Iterate through the children of the token\n    for child in token.children:\n        # If the child is the nominal subject (nsubj) and not a stop word\n        if child.dep_ == \"nsubj\" and not child.is_stop:\n            A = child.text\n\n        # If the child is the direct object (dobj) and its part-of-speech is ADJ (adjective), and not a stop word\n        if child.dep_ == \"dobj\" and child.pos_ == \"ADJ\" and not child.is_stop:\n            M = child.text\n    \n            # If the child is a negation word\n        if child.dep_ == \"neg\":\n            neg_prefix = child.text\n            add_neg_pfx = True\n            \n        # If negation is present, add the negation prefix to the sentiment modifier (M)\n        if add_neg_pfx and M != \"999999\":\n            M = f\"{neg_prefix} {M}\"\n        \n        # If both aspect (A) and sentiment modifier (M) are valid, format the result\n        if A != \"999999\" and M != \"999999\":\n            # If aspect is one of the specified pronouns, replace it with \"product\"\n            if A in prod_pronouns:\n                A = \"product\"\n            return [{\"noun\": A, \"adj\": M, \"rule\": 2}]\n\n        # If no valid aspect-sentiment pair is found, return an empty list\n        return []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rule3(token, prod_pronouns):\n    \"\"\"\n    Apply Rule 3: Extract aspect and its corresponding description from the review.\n    Adjectival Complement - A is a child of something with a relationship of nsubj, while M is a child of the same something with a relationship of acomp.\n\n    Args:\n        token (spacy.tokens.Token): The token to analyze.\n        prod_pronouns (list): List of product pronouns.\n\n    Returns:\n        list: A list of dictionaries containing aspect, sentiment modifier, and rule number (3).\n    \"\"\"\n    A, M = \"999999\", \"999999\"\n    add_neg_pfx = False\n\n    for child in token.children:\n        # Check if the child is the nominal subject (nsubj) and not a stop word\n        if child.dep_ == \"nsubj\" and not child.is_stop:\n            A = child.text\n\n        # Check if the child is the adjectival complement (acomp) and not a stop word\n        if child.dep_ == \"acomp\" and not child.is_stop:\n            M = child.text\n\n        # Example: 'this could have been better' -> (this, not better)\n        # If a child is an auxiliary (aux) with the tag \"MD\"  - Modal Auxiliary or a negation word (MD = \"can,\" \"could,\" \"will,\" \"would,\" \"shall,\" \"should,\" \"may,\" \"might,\" and \"must.\")\n        if child.dep_ == \"aux\" and child.tag_ == \"MD\":\n            neg_prefix = \"not\"\n            add_neg_pfx = True\n\n        # Check if the child is a negation word\n        if child.dep_ == \"neg\":\n            neg_prefix = child.text\n            add_neg_pfx = True\n\n    # If negation is present, add the negation prefix to the sentiment modifier (M)\n    if add_neg_pfx and M != \"999999\":\n        M = f\"{neg_prefix} {M}\"\n\n    # If both aspect (A) and sentiment modifier (M) are valid, format the result\n    if A != \"999999\" and M != \"999999\":\n        # If aspect is one of the specified pronouns, replace it with \"product\"\n        if A in prod_pronouns:\n            A = \"product\"\n        return [{\"noun\": A, \"adj\": M, \"rule\": 3}]\n\n    # If no valid aspect-sentiment pair is found, return an empty list\n    return []\n","metadata":{},"execution_count":null,"outputs":[]}]}