{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# split the review into multiple sentences based on custom delimiters","metadata":{}},{"cell_type":"markdown","source":"```python\ndef split_review_custom_delimiters(text):\n    \"\"\"\n    This function splits the review into multiple sentences based on custom delimiters.\n    \"\"\"\n    delimiters = \".\", \"but\", \"and\", \"also\"\n    escaped_delimiters = map(re.escape, delimiters) # Result: ['\\\\.', 'but', 'and', 'also']\n    regex_pattern = '|'.join(escaped_delimiters) # Applying the custom delimiters # Result: '\\\\.|but|and|also'\n    splitted = re.split(regex_pattern, text) # Splitting the review function from the re module to split the input text into a list of substrings based on the specified regular expression pattern.\n    return[sentence.strip() for sentence in splitted if sentence.strip()] #this line ensures that only non-empty sentences (after stripping whitespaces) are included in the final result.  sentence.strip(): Strips any leading or trailing whitespaces from the sentence.\n\n\ntext_input = \"This is a sample text. It includes some details, but not everything. And also, there are additional points.\"\nsplitted = re.split(regex_pattern, text_input)\nprint(splitted)\n\n['This is a sample text', ' It includes some details, ', ' not everything', ' ', ' there are additional points', '']\n\n```\n\n","metadata":{}},{"cell_type":"markdown","source":"# WordNetLemmatizer and stopwords","metadata":{}},{"cell_type":"markdown","source":"It appears that you are using the `WordNetLemmatizer` and `stopwords` from the Natural Language Toolkit (nltk) library. Here's a brief explanation of each:\n\n1. **WordNetLemmatizer:**\n   - The `WordNetLemmatizer` is part of the NLTK library and is used for lemmatization, which is the process of reducing words to their base or root form.\n   - Lemmatization helps in standardizing words, so different forms of a word are treated as the same.\n\n   Example:\n   ```python\n   from nltk.stem import WordNetLemmatizer\n\n   lemma = WordNetLemmatizer()\n   word = \"running\"\n   lemmatized_word = lemma.lemmatize(word, pos='v')  # 'v' specifies the part of speech, in this case, verb\n   print(lemmatized_word)  # Output: 'run'\n   ```\n\n2. **stopwords:**\n   - The `stopwords` corpus from NLTK contains common words that are often removed from text during text preprocessing.\n   - These words (like 'and', 'the', 'is', etc.) are considered as noise in many natural language processing tasks.\n\n   Example:\n   ```python\n   from nltk.corpus import stopwords\n\n   all_stopwords = set(stopwords.words('english'))\n   sentence = \"This is an example sentence with some stop words.\"\n   words = sentence.split()\n   filtered_words = [word for word in words if word.lower() not in all_stopwords]\n   print(filtered_words)\n   ```\n\n   This will print: `['example', 'sentence', 'stop', 'words.']`, as common English stopwords are removed.\n\nMake sure you have the NLTK library installed (`pip install nltk`) and have downloaded the necessary resources (you can download stopwords using `nltk.download('stopwords')`).","metadata":{}},{"cell_type":"markdown","source":"# r prefix","metadata":{}},{"cell_type":"markdown","source":"Yes, you can use the regular expression without the `r` prefix, but it's a good practice to include it. The `r` prefix denotes a raw string in Python, and it's commonly used with regular expressions to ensure that backslashes are treated literally.\n\nFor example, both of the following lines are equivalent:\n\n```python\nstatement = re.sub(r'[^a-zA-Z\\s]', ' ', statement)\n```\n\n```python\nstatement = re.sub('[^a-zA-Z\\\\s]', ' ', statement)\n```\n\nUsing the `r` prefix is recommended to avoid potential issues with backslashes in regular expressions.","metadata":{}},{"cell_type":"markdown","source":"# nltk.download()","metadata":{}},{"cell_type":"markdown","source":"The `nltk.download()` function is used to download various corpora, models, and other linguistic data that NLTK (Natural Language Toolkit) uses. In your specific case, you are downloading two specific resources:\n\n1. **WordNet:**\n   - WordNet is a lexical database of the English language. It groups English words into sets of synonyms called synsets, provides short definitions, and records the relationships between these synsets.\n\n2. **Open Multilingual Wordnet (OMW) version 1.4:**\n   - Open Multilingual Wordnet is an extension of WordNet that includes synsets for multiple languages. Version 1.4 is a specific version of the Open Multilingual Wordnet.\n\nBy downloading these resources, you gain access to a rich set of lexical and linguistic data that can be useful for various natural language processing (NLP) tasks, such as lemmatization, synonym analysis, and multilingual language processing.\n\nIf you're working on projects involving text analysis, sentiment analysis, machine learning, or any other NLP-related tasks using NLTK, having these resources locally allows your code to access and utilize them efficiently.","metadata":{}},{"cell_type":"markdown","source":"#  the differences between using `enumerate` and a regular `for` loop without `enumerate` in the context of iterating through a sequence like a list or array.\n\n### Using `enumerate`:\n\n```python\nfor i, review_text in enumerate(df[\"Review\"].values):\n    # Apply the splitting function to break down the review\n    review_split = split_review(review_text)\n```\n\n1. **Access to Index (`i`):** `enumerate` provides an index (`i`) along with the value (`review_text`) during each iteration. This is useful when you need to know the position of the item in the sequence.\n\n2. **Readability:** It can make the code more readable, especially when the index is needed within the loop.\n\n### Without `enumerate`:\n\n```python\nfor i in range(len(df[\"Review\"].values)):\n    review_text = df[\"Review\"].values[i]\n\n    # Apply the splitting function to break down the review\n    review_split = split_review(review_text)\n```\n\n1. **Manual Indexing:** You need to manually use the index (`i`) to access the value from the sequence. This approach is more verbose.\n\n2. **Index Usage:** If the index is not needed within the loop, this approach might be simpler.\n\n### Recommendations:\n\n- **Use `enumerate` when:** You need both the index and the value during the loop, or you want cleaner and more readable code.\n\n- **Use without `enumerate` when:** You don't need the index within the loop, and you prefer a simpler syntax.\n\nIn your specific case, since you're not using the index within the loop, you can choose either method based on personal preference or code style conventions. The `enumerate` method might be considered more Pythonic and is often preferred when the index is not used.","metadata":{}},{"cell_type":"markdown","source":"# Aspect Extraction","metadata":{}},{"cell_type":"markdown","source":"\"Aspect Extraction\" refers to the process of identifying and isolating specific features, topics, or components within a given text or dataset. In natural language processing (NLP), this task involves recognizing and extracting elements that carry significant meaning, such as key aspects, themes, or attributes discussed in a document or a set of documents. The goal is to automatically identify and capture important information relevant to a particular domain or context, enabling a more focused and structured understanding of the content.","metadata":{}},{"cell_type":"markdown","source":"# an example of using `spacy` and `displacy` for visualizing the dependency parse tree and named entity recognition (NER) annotations.\n\n```python\nimport spacy\nfrom spacy import displacy\n\n# Load the English NLP model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example sentence\nsentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n\n# Process the sentence using spaCy\ndoc = nlp(sentence)\n\n# Visualize the dependency parse tree\ndisplacy.render(doc, style=\"dep\", options={'distance': 90})\n\n# Visualize the named entity recognition (NER) annotations\ndisplacy.render(doc, style=\"ent\")\n```\n\nIn this example, we first load the English NLP model (`en_core_web_sm`). We then process a sample sentence using spaCy. Finally, we use `displacy.render` to visualize the dependency parse tree and named entity recognition (NER) annotations.\n\nThe `options` parameter in `displacy.render` is used to control the visual appearance of the dependency parse tree.\n\nthe output would look like:\n\n1. **Dependency Parse Tree Visualization (`displacy.render` with style=\"dep\"):**\n   - The output will be a visual representation of the dependency parse tree for the given sentence.\n   - Each word in the sentence will be displayed with arrows connecting them based on their syntactic dependencies.\n   - You'll see labels indicating the type of dependency (e.g., \"nsubj\" for nominal subject, \"ROOT\" for the root of the tree, etc.).\n\n2. **Named Entity Recognition (NER) Visualization (`displacy.render` with style=\"ent\"):**\n   - The output will be a visual representation of named entities identified in the sentence.\n   - Named entities such as organizations, locations, and monetary values will be highlighted and labeled.\n\nTo view the visualizations, you might need to run this code in an environment that supports rendering HTML, like a Jupyter notebook or a web-based Python environment. If you run this in a Python script, you might want to save the visualizations as HTML and open them in a web browser. For example:\n\n```python\n# Save dependency parse tree visualization as HTML\ndisplacy.serve(doc, style=\"dep\", options={'distance': 90})\n\n# Save named entity recognition visualization as HTML\ndisplacy.serve(doc, style=\"ent\")\n```\n","metadata":{}},{"cell_type":"markdown","source":"# [spaCy](https://spacy.io/)\n\nis an open-source natural language processing (NLP) library designed for various NLP tasks. It provides pre-trained models and utilities for processing and analyzing text in a fast and efficient way. Some of the key features and capabilities of spaCy include:\n\n1. **Tokenization:** Breaking down text into individual words or tokens.\n\n2. **Part-of-Speech (POS) Tagging:** Assigning grammatical parts of speech to each word in a sentence (e.g., noun, verb, adjective).\n\n3. **Named Entity Recognition (NER):** Identifying entities such as persons, organizations, locations, dates, and more in the text.\n\n4. **Dependency Parsing:** Analyzing the syntactic structure of a sentence by determining the relationships between words.\n\n5. **Lemmatization:** Reducing words to their base or root form (e.g., \"running\" to \"run\").\n\n6. **Sentence Boundary Detection (SBD):** Identifying sentence boundaries in a text.\n\n7. **Word Embeddings:** Representing words as vectors in a high-dimensional space, allowing for semantic similarity analysis.\n\n8. **Text Classification:** Assigning predefined categories or labels to text based on its content.\n\n9. **Rule-based Matching:** Defining and applying rules to extract information from text.\n\n10. **Integration with Deep Learning:** spaCy can be used in conjunction with deep learning frameworks for more advanced NLP tasks.\n\nIt's a versatile tool used by researchers, developers, and data scientists for a wide range of applications, including information extraction, sentiment analysis, chatbot development, and more. The library is known for its efficiency, accuracy, and ease of use.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}