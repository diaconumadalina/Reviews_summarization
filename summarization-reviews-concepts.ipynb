{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# split the review into multiple sentences based on custom delimiters","metadata":{}},{"cell_type":"markdown","source":"```python\ndef split_review_custom_delimiters(text):\n    \"\"\"\n    This function splits the review into multiple sentences based on custom delimiters.\n    \"\"\"\n    delimiters = \".\", \"but\", \"and\", \"also\"\n    escaped_delimiters = map(re.escape, delimiters) # Result: ['\\\\.', 'but', 'and', 'also']\n    regex_pattern = '|'.join(escaped_delimiters) # Applying the custom delimiters # Result: '\\\\.|but|and|also'\n    splitted = re.split(regex_pattern, text) # Splitting the review function from the re module to split the input text into a list of substrings based on the specified regular expression pattern.\n    return[sentence.strip() for sentence in splitted if sentence.strip()] #this line ensures that only non-empty sentences (after stripping whitespaces) are included in the final result.  sentence.strip(): Strips any leading or trailing whitespaces from the sentence.\n\n\ntext_input = \"This is a sample text. It includes some details, but not everything. And also, there are additional points.\"\nsplitted = re.split(regex_pattern, text_input)\nprint(splitted)\n\n['This is a sample text', ' It includes some details, ', ' not everything', ' ', ' there are additional points', '']\n\n```\n\n","metadata":{}},{"cell_type":"markdown","source":"# WordNetLemmatizer and stopwords","metadata":{}},{"cell_type":"markdown","source":"It appears that you are using the `WordNetLemmatizer` and `stopwords` from the Natural Language Toolkit (nltk) library. Here's a brief explanation of each:\n\n1. **WordNetLemmatizer:**\n   - The `WordNetLemmatizer` is part of the NLTK library and is used for lemmatization, which is the process of reducing words to their base or root form.\n   - Lemmatization helps in standardizing words, so different forms of a word are treated as the same.\n\n   Example:\n   ```python\n   from nltk.stem import WordNetLemmatizer\n\n   lemma = WordNetLemmatizer()\n   word = \"running\"\n   lemmatized_word = lemma.lemmatize(word, pos='v')  # 'v' specifies the part of speech, in this case, verb\n   print(lemmatized_word)  # Output: 'run'\n   ```\n\n2. **stopwords:**\n   - The `stopwords` corpus from NLTK contains common words that are often removed from text during text preprocessing.\n   - These words (like 'and', 'the', 'is', etc.) are considered as noise in many natural language processing tasks.\n\n   Example:\n   ```python\n   from nltk.corpus import stopwords\n\n   all_stopwords = set(stopwords.words('english'))\n   sentence = \"This is an example sentence with some stop words.\"\n   words = sentence.split()\n   filtered_words = [word for word in words if word.lower() not in all_stopwords]\n   print(filtered_words)\n   ```\n\n   This will print: `['example', 'sentence', 'stop', 'words.']`, as common English stopwords are removed.\n\nMake sure you have the NLTK library installed (`pip install nltk`) and have downloaded the necessary resources (you can download stopwords using `nltk.download('stopwords')`).","metadata":{}},{"cell_type":"markdown","source":"# r prefix","metadata":{}},{"cell_type":"markdown","source":"Yes, you can use the regular expression without the `r` prefix, but it's a good practice to include it. The `r` prefix denotes a raw string in Python, and it's commonly used with regular expressions to ensure that backslashes are treated literally.\n\nFor example, both of the following lines are equivalent:\n\n```python\nstatement = re.sub(r'[^a-zA-Z\\s]', ' ', statement)\n```\n\n```python\nstatement = re.sub('[^a-zA-Z\\\\s]', ' ', statement)\n```\n\nUsing the `r` prefix is recommended to avoid potential issues with backslashes in regular expressions.","metadata":{}},{"cell_type":"markdown","source":"# nltk.download()","metadata":{}},{"cell_type":"markdown","source":"The `nltk.download()` function is used to download various corpora, models, and other linguistic data that NLTK (Natural Language Toolkit) uses. In your specific case, you are downloading two specific resources:\n\n1. **WordNet:**\n   - WordNet is a lexical database of the English language. It groups English words into sets of synonyms called synsets, provides short definitions, and records the relationships between these synsets.\n\n2. **Open Multilingual Wordnet (OMW) version 1.4:**\n   - Open Multilingual Wordnet is an extension of WordNet that includes synsets for multiple languages. Version 1.4 is a specific version of the Open Multilingual Wordnet.\n\nBy downloading these resources, you gain access to a rich set of lexical and linguistic data that can be useful for various natural language processing (NLP) tasks, such as lemmatization, synonym analysis, and multilingual language processing.\n\nIf you're working on projects involving text analysis, sentiment analysis, machine learning, or any other NLP-related tasks using NLTK, having these resources locally allows your code to access and utilize them efficiently.","metadata":{}},{"cell_type":"markdown","source":"#  the differences between using `enumerate` and a regular `for` loop without `enumerate` in the context of iterating through a sequence like a list or array.\n\n### Using `enumerate`:\n\n```python\nfor i, review_text in enumerate(df[\"Review\"].values):\n    # Apply the splitting function to break down the review\n    review_split = split_review(review_text)\n```\n\n1. **Access to Index (`i`):** `enumerate` provides an index (`i`) along with the value (`review_text`) during each iteration. This is useful when you need to know the position of the item in the sequence.\n\n2. **Readability:** It can make the code more readable, especially when the index is needed within the loop.\n\n### Without `enumerate`:\n\n```python\nfor i in range(len(df[\"Review\"].values)):\n    review_text = df[\"Review\"].values[i]\n\n    # Apply the splitting function to break down the review\n    review_split = split_review(review_text)\n```\n\n1. **Manual Indexing:** You need to manually use the index (`i`) to access the value from the sequence. This approach is more verbose.\n\n2. **Index Usage:** If the index is not needed within the loop, this approach might be simpler.\n\n### Recommendations:\n\n- **Use `enumerate` when:** You need both the index and the value during the loop, or you want cleaner and more readable code.\n\n- **Use without `enumerate` when:** You don't need the index within the loop, and you prefer a simpler syntax.\n\nIn your specific case, since you're not using the index within the loop, you can choose either method based on personal preference or code style conventions. The `enumerate` method might be considered more Pythonic and is often preferred when the index is not used.","metadata":{}}]}